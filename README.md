# RAG Chatbot

A modern web-based chatbot using Retrieval-Augmented Generation (RAG) architecture with support for multiple LLM providers.

## Features

- ü§ñ **RAG Architecture**: Retrieval-Augmented Generation for context-aware responses
- üìÑ **Multi-format Support**: Upload and process PDF, DOCX, and TXT files
- üß† **Multiple LLM Providers**: 
  - Google Gemini (Cloud)
  - Local models via LM Studio (phi-2, Gemma, etc.)
- üé® **Modern UI**: Clean, responsive interface with dark mode support
- üìä **Vector Storage**: Chroma vector store for efficient document retrieval
- üîç **Document Management**: Upload, view, and manage your knowledge base

## Architecture

```
RAG_ChatBot_01/
‚îú‚îÄ‚îÄ main.py                 # Flask application entry point
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ env.example            # Environment variables template
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ llm_provider.py    # LLM provider management
‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py # Document processing
‚îÇ   ‚îî‚îÄ‚îÄ vector_store.py    # Vector store operations
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html         # Web interface
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ uploads/           # Uploaded files
    ‚îî‚îÄ‚îÄ vectorstore/       # Chroma vector store
```

## Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd RAG_ChatBot_01
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp env.example .env
   # Edit .env with your API keys and configuration
   ```

## Configuration

### Environment Variables

Create a `.env` file with the following variables:

```env
# Flask Configuration
SECRET_KEY=your-secret-key-here
FLASK_ENV=development

# Google Gemini API (Optional)
GOOGLE_API_KEY=your-google-api-key-here

# Local LLM Configuration (Optional)
LOCAL_LLM_ENDPOINT=http://localhost:1234/v1/chat/completions
LOCAL_MODEL_NAME=phi-2

# Vector Store Configuration
VECTOR_STORE_PATH=data/vectorstore

# Upload Configuration
MAX_FILE_SIZE=16777216
UPLOAD_FOLDER=data/uploads
```

### LLM Setup

#### Google Gemini
1. Get your API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Add it to your `.env` file

#### Local LLM (LM Studio)
1. Download and install [LM Studio](https://lmstudio.ai/)
2. Load a model (e.g., phi-2, Gemma-2b)
3. Start the local server (usually runs on `http://localhost:1234`)
4. Configure the endpoint in your `.env` file

## Usage

1. **Start the application**
   ```bash
   python main.py
   ```

2. **Open your browser**
   Navigate to `http://localhost:5000`

3. **Upload documents**
   - Click the upload area in the sidebar
   - Select PDF, DOCX, or TXT files
   - Documents will be processed and added to the vector store

4. **Start chatting**
   - Choose your preferred LLM (Gemini or Local)
   - Ask questions about your uploaded documents
   - The chatbot will retrieve relevant context and generate responses

## API Endpoints

- `GET /` - Main application page
- `POST /upload` - Upload and process documents
- `POST /chat` - Send chat messages
- `GET /documents` - Get list of uploaded documents
- `GET /history` - Get chat history
- `POST /clear-history` - Clear chat history

## Features in Detail

### Document Processing
- **PDF**: Uses PyMuPDF for text extraction
- **DOCX**: Uses python-docx for document parsing
- **TXT**: Direct text file processing
- **Chunking**: Documents are split into manageable chunks for better retrieval

### Vector Store
- **Chroma**: Local vector database for document embeddings
- **Embeddings**: Uses sentence-transformers for text embeddings
- **Persistence**: Vector store is saved locally and persists between sessions

### LLM Integration
- **Gemini**: Cloud-based model with high performance
- **Local**: Self-hosted models for privacy and offline use
- **Context Injection**: Relevant document chunks are included in prompts

### User Interface
- **Responsive Design**: Works on desktop and mobile
- **Dark Mode**: Toggle between light and dark themes
- **Real-time Chat**: Instant message exchange
- **File Management**: Visual document upload and management

## Development

### Project Structure
- `main.py`: Flask application with routes
- `backend/llm_provider.py`: LLM provider abstraction
- `backend/document_loader.py`: Document processing utilities
- `backend/vector_store.py`: Vector store operations
- `templates/index.html`: Frontend interface

### Adding New Features
1. **New LLM Provider**: Extend `LLMProvider` class
2. **New Document Type**: Add parser to `DocumentLoader`
3. **New Vector Store**: Implement vector store interface
4. **UI Enhancements**: Modify `templates/index.html`

## Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   pip install -r requirements.txt
   ```

2. **Chroma Connection Issues**
   - Ensure write permissions to `data/vectorstore/`
   - Delete the directory to reset the vector store

3. **Local LLM Not Responding**
   - Check if LM Studio is running
   - Verify the endpoint URL in `.env`
   - Ensure the model is loaded in LM Studio

4. **Gemini API Errors**
   - Verify your API key is correct
   - Check API quota and billing status

### Logs
The application logs errors and operations to the console. Check the terminal output for debugging information.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- [LangChain](https://langchain.com/) for the RAG framework
- [Chroma](https://www.trychroma.com/) for vector storage
- [LM Studio](https://lmstudio.ai/) for local LLM hosting
- [Google Gemini](https://ai.google.dev/) for cloud LLM access 



## C√°ch ho·∫°t ƒë·ªông c·ªßa web app

### 1. Upload v√† x·ª≠ l√Ω t√†i li·ªáu
- Ng∆∞·ªùi d√πng upload file PDF, DOCX, TXT qua giao di·ªán web.
- Backend l∆∞u file v√†o th∆∞ m·ª•c `data/uploads/`.
- File ƒë∆∞·ª£c ƒë·ªçc v√† chia nh·ªè th√†nh c√°c chunk (theo c·∫•u h√¨nh chunk_size, chunk_overlap).

### 2. Sinh embedding v√† l∆∞u vectorstore
- M·ªói chunk ƒë∆∞·ª£c chuy·ªÉn th√†nh vector embedding b·∫±ng model (v√≠ d·ª•: `intfloat/multilingual-e5-large`).
- C√°c embedding v√† metadata (t√™n file, v·ªã tr√≠, lo·∫°i file, ...) ƒë∆∞·ª£c l∆∞u v√†o vector database (ChromaDB) trong `data/vectorstore/`.

### 3. Truy v·∫•n v√† sinh c√¢u tr·∫£ l·ªùi
- Khi ng∆∞·ªùi d√πng g·ª≠i c√¢u h·ªèi:
  1. Backend l·∫•y 12 chunk li√™n quan nh·∫•t (theo embedding) t·ª´ vectorstore.
  2. T·ª± ƒë·ªông t√¨m th√™m c√°c chunk ch·ª©a t·ª´ kh√≥a ƒë·∫∑c bi·ªát trong c√¢u h·ªèi (v√≠ d·ª•: "208HV", "NMLD") ƒë·ªÉ gh√©p v√†o context.
  3. L·∫•y 10 l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t t·ª´ session ƒë·ªÉ truy·ªÅn v√†o prompt.
  4. Gh√©p context (c√≥ c·∫Øt chunk t·ªëi ƒëa 2000 k√Ω t·ª±), l·ªãch s·ª≠ h·ªôi tho·∫°i, v√† c√¢u h·ªèi th√†nh prompt.
  5. G·ª≠i prompt n√†y l√™n LLM (Gemini ho·∫∑c local LLM qua LM Studio).
  6. Nh·∫≠n c√¢u tr·∫£ l·ªùi, format HTML ƒë·∫πp.
  7. Tr·∫£ v·ªÅ frontend c√πng danh s√°ch ngu·ªìn (file) duy nh·∫•t.

### 4. Hi·ªÉn th·ªã v√† qu·∫£n l√Ω tr√™n frontend
- Giao di·ªán chat hi·ªán ƒë·∫°i, h·ªó tr·ª£:
  - G·ª≠i tin nh·∫Øn b·∫±ng Enter, xu·ªëng d√≤ng b·∫±ng Shift+Enter.
  - Ch·ªçn LLM, model Gemini ƒë·ªông.
  - Upload t√†i li·ªáu, xem danh s√°ch t√†i li·ªáu ƒë√£ upload.
  - Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i, ngu·ªìn t√†i li·ªáu li√™n quan.
- Trang admin:
  - Xem to√†n b·ªô chunk trong vectorstore, click ƒë·ªÉ xem chi ti·∫øt n·ªôi dung v√† metadata.
  - X√≥a to√†n b·ªô vectorstore ho·∫∑c t·ª´ng t√†i li·ªáu.
  - Xem API docs, th·ª≠ API tr·ª±c ti·∫øp.

### 5. API v√† qu·∫£n tr·ªã
- C√≥ trang API Explorer (`/api-docs`) t·ª± ƒë·ªông li·ªát k√™ v√† cho ph√©p th·ª≠ c√°c endpoint.
- C√°c API ch√≠nh: upload, chat, l·∫•y danh s√°ch t√†i li·ªáu, debug vectorstore, x√≥a vectorstore, x√≥a t√†i li·ªáu, l·∫•y l·ªãch s·ª≠ chat, ...

### 6. L·ªãch s·ª≠ h·ªôi tho·∫°i v√† session
- L·ªãch s·ª≠ h·ªôi tho·∫°i ƒë∆∞·ª£c l∆∞u trong session Flask (cookie).
- Khi refresh ho·∫∑c ƒë·ªïi session, l·ªãch s·ª≠ s·∫Ω b·ªã x√≥a.
- Khi g·ª≠i c√¢u h·ªèi, 10 l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t s·∫Ω ƒë∆∞·ª£c truy·ªÅn v√†o prompt ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh.

### 7. T·ªëi ∆∞u v√† b·∫£o tr√¨
- C√≥ th·ªÉ ƒë·ªïi model embedding, chunk_size, chunk_overlap trong code/config.
- C√≥ th·ªÉ xem/log prompt g·ª≠i l√™n LLM ƒë·ªÉ debug.
- C√≥ th·ªÉ m·ªü r·ªông th√™m c√°c ch·ª©c nƒÉng qu·∫£n tr·ªã, t√¨m ki·∫øm, ph√¢n quy·ªÅn, ...