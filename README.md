# RAG Chatbot

A modern web-based chatbot using Retrieval-Augmented Generation (RAG) architecture with support for multiple LLM providers.

## Features

- ğŸ¤– **RAG Architecture**: Retrieval-Augmented Generation for context-aware responses
- ğŸ“„ **Multi-format Support**: Upload and process PDF, DOCX, and TXT files
- ğŸ§  **Multiple LLM Providers**: 
  - Google Gemini (Cloud)
  - Local models via LM Studio (phi-2, Gemma, etc.)
- ğŸ¨ **Modern UI**: Clean, responsive interface with dark mode support, beautiful chat bubbles, avatars, typing status, and real-time progress
- ğŸ“Š **Vector Storage**: Chroma vector store for efficient document retrieval
- ğŸ” **Document Management**: Upload, view, and manage your knowledge base
- ğŸ› ï¸ **Admin Dashboard**: View/delete vectorstore, manage uploads, view chunk details
- ğŸ§© **API Explorer**: Interactive API docs and testing
- ğŸŒ **Vietnamese default answers**: Optimized for Vietnamese context
- ğŸ“ **Prompt logging, source deduplication, multi-chunk synthesis**

## Project Structure

```
RAG_ChatBot_01/
â”œâ”€â”€ main.py                 # Flask application entry point, all routes
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ env.example            # Environment variables template
â”œâ”€â”€ config.py               # App configuration
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_provider.py    # LLM provider management (Gemini, Local, ...)
â”‚   â”œâ”€â”€ document_loader.py # Document processing, chunking, file parsing
â”‚   â””â”€â”€ vector_store.py    # Vector store operations (ChromaDB)
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html         # Main chat UI
â”‚   â”œâ”€â”€ admin.html         # Admin dashboard (view/delete DB, chunk details)
â”‚   â”œâ”€â”€ api_docs.html      # API Explorer (Swagger-like)
â”‚   â””â”€â”€ test_upload.html   # (Optional) Test upload UI
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css          # All CSS (modern, responsive, dark mode)
â”‚   â””â”€â”€ main.js            # All JS (chat, upload, progress, admin, ...)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/           # Uploaded files
â”‚   â”œâ”€â”€ vectorstore/       # Chroma vector store (persisted DB)
â”‚   â””â”€â”€ sample/            # Sample documents for testing
â”‚       â”œâ”€â”€ ai_technologies.txt
â”‚       â””â”€â”€ sample_document.txt
â”œâ”€â”€ logoBSR.png            # Company logo (used in sidebar, AI avatar)
â””â”€â”€ README.md
```

## How it works (Pipeline)

### 1. Upload & Process Documents
- User uploads PDF, DOCX, or TXT files via the web UI.
- Backend saves files to `data/uploads/`.
- Each file is parsed and split into text chunks (configurable chunk size/overlap).

### 2. Embedding & Vectorstore
- Each chunk is embedded using a model (e.g. `intfloat/multilingual-e5-large`).
- Embeddings + metadata (file name, position, ...) are stored in ChromaDB (`data/vectorstore/`).

### 3. Chat & Retrieval
- User sends a question via chat UI.
- Backend retrieves top relevant chunks (semantic search) from vectorstore.
- Special keyword chunks are auto-merged for context.
- Last 10 chat turns are included for context.
- Prompt is constructed (context + history + question) and sent to LLM (Gemini or Local).
- LLM response is returned, formatted, and sources are deduplicated.

### 4. Frontend Display & Management
- Modern chat UI: chat bubbles, avatars, typing status, Enter to send, Shift+Enter for newline.
- Upload progress bar (with chunking progress), cancel/continue on page close.
- Sidebar: LLM/model selection, document/chunk count, DB/model status, clear all button.
- Admin dashboard: view/delete vectorstore, see all chunks, chunk details, delete by document.
- API Explorer: interactive docs and live API testing.

### 5. API & Admin
- `/api-docs`: API Explorer (auto-generated docs, live test)
- `/admin`: Admin dashboard (view/delete DB, chunk details)
- Main APIs: upload, chat, documents, vectorstore status, clear vectorstore, delete document, chat history, ...

### 6. Session & History
- Chat history is stored in Flask session (cookie-based, per user).
- Last 10 turns are used for context in prompt.
- History is cleared on refresh or new session.

## Sample Data
- Sample files in `data/sample/` for quick testing.
- You can upload these to see how the system works.

## Installation & Usage

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd RAG_ChatBot_01
   ```
2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
4. **Set up environment variables**
   ```bash
   cp env.example .env
   # Edit .env with your API keys and configuration
   ```
5. **Start the application**
   ```bash
   python main.py
   ```
6. **Open your browser**
   - Go to `http://localhost:5000`

## Configuration

- All config via `.env` or `config.py` (see `env.example` for all options)
- Key options:
  - `GOOGLE_API_KEY`: Gemini API key
  - `LOCAL_LLM_ENDPOINT`: LM Studio endpoint
  - `VECTOR_STORE_PATH`: Path to ChromaDB
  - `UPLOAD_FOLDER`: Where uploads are stored
  - `MAX_FILE_SIZE`: Max upload size (default 50MB)

## API Endpoints (Main)
- `GET /` - Main chat UI
- `POST /upload` - Upload and process documents (returns doc_id, triggers chunking)
- `GET /processing-status?doc_id=...` - Get chunking progress
- `POST /chat` - Chat with RAG bot
- `GET /documents` - List uploaded documents
- `POST /clear-vectorstore` - Delete all vectorstore data
- `GET /vectorstore-status` - Get DB/model status, doc/chunk count
- `GET /history` - Get chat history
- `POST /clear-history` - Clear chat history

## UI/UX Highlights
- **Modern chat bubbles**: Wide, readable, responsive, source files shown below each answer
- **Progress bar**: Shows both upload and chunking progress, with cancel/continue on page close
- **Sidebar**: LLM/model selection, DB/model status, document/chunk count, clear all
- **Admin dashboard**: View/delete vectorstore, see all chunks, chunk details, delete by document
- **API Explorer**: Live API docs and testing
- **Dark mode**: Toggle with one click
- **Company logo**: Sidebar and AI avatar
- **Vietnamese default answers**: Optimized for Vietnamese context

## Development & Customization
- Add new LLM: Extend `backend/llm_provider.py`
- Add new document type: Extend `backend/document_loader.py`
- Change chunking/embedding: Edit `config.py` or `document_loader.py`
- UI/UX: Edit `templates/index.html`, `static/style.css`, `static/main.js`
- Admin/API: Edit `templates/admin.html`, `templates/api_docs.html`

## Troubleshooting
- **Upload lá»—i 413**: TÄƒng `MAX_CONTENT_LENGTH` trong Flask vÃ  proxy (Nginx/Apache)
- **KhÃ´ng nháº­n model local**: Kiá»ƒm tra LM Studio Ä‘Ã£ cháº¡y vÃ  endpoint Ä‘Ãºng
- **Gemini lá»—i 404/401**: Kiá»ƒm tra API key vÃ  quota
- **Vectorstore lá»—i**: XÃ³a thÆ° má»¥c `data/vectorstore/` Ä‘á»ƒ reset

## License
MIT License

## Acknowledgments
- [LangChain](https://langchain.com/) for RAG framework
- [Chroma](https://www.trychroma.com/) for vector storage
- [LM Studio](https://lmstudio.ai/) for local LLM hosting
- [Google Gemini](https://ai.google.dev/) for cloud LLM access

## CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a web app

### 1. Upload vÃ  xá»­ lÃ½ tÃ i liá»‡u
- NgÆ°á»i dÃ¹ng upload file PDF, DOCX, TXT qua giao diá»‡n web.
- Backend lÆ°u file vÃ o thÆ° má»¥c `data/uploads/`.
- File Ä‘Æ°á»£c Ä‘á»c vÃ  chia nhá» thÃ nh cÃ¡c chunk (theo cáº¥u hÃ¬nh chunk_size, chunk_overlap).

### 2. Sinh embedding vÃ  lÆ°u vectorstore
- Má»—i chunk Ä‘Æ°á»£c chuyá»ƒn thÃ nh vector embedding báº±ng model (vÃ­ dá»¥: `intfloat/multilingual-e5-large`).
- CÃ¡c embedding vÃ  metadata (tÃªn file, vá»‹ trÃ­, loáº¡i file, ...) Ä‘Æ°á»£c lÆ°u vÃ o vector database (ChromaDB) trong `data/vectorstore/`.

### 3. Truy váº¥n vÃ  sinh cÃ¢u tráº£ lá»i
- Khi ngÆ°á»i dÃ¹ng gá»­i cÃ¢u há»i:
  1. Backend láº¥y 12 chunk liÃªn quan nháº¥t (theo embedding) tá»« vectorstore.
  2. Tá»± Ä‘á»™ng tÃ¬m thÃªm cÃ¡c chunk chá»©a tá»« khÃ³a Ä‘áº·c biá»‡t trong cÃ¢u há»i (vÃ­ dá»¥: "208HV", "NMLD") Ä‘á»ƒ ghÃ©p vÃ o context.
  3. Láº¥y 10 lÆ°á»£t há»™i thoáº¡i gáº§n nháº¥t tá»« session Ä‘á»ƒ truyá»n vÃ o prompt.
  4. GhÃ©p context (cÃ³ cáº¯t chunk tá»‘i Ä‘a 2000 kÃ½ tá»±), lá»‹ch sá»­ há»™i thoáº¡i, vÃ  cÃ¢u há»i thÃ nh prompt.
  5. Gá»­i prompt nÃ y lÃªn LLM (Gemini hoáº·c local LLM qua LM Studio).
  6. Nháº­n cÃ¢u tráº£ lá»i, format HTML Ä‘áº¹p.
  7. Tráº£ vá» frontend cÃ¹ng danh sÃ¡ch nguá»“n (file) duy nháº¥t.

### 4. Hiá»ƒn thá»‹ vÃ  quáº£n lÃ½ trÃªn frontend
- Giao diá»‡n chat hiá»‡n Ä‘áº¡i, há»— trá»£:
  - Gá»­i tin nháº¯n báº±ng Enter, xuá»‘ng dÃ²ng báº±ng Shift+Enter.
  - Chá»n LLM, model Gemini Ä‘á»™ng.
  - Upload tÃ i liá»‡u, xem danh sÃ¡ch tÃ i liá»‡u Ä‘Ã£ upload.
  - Hiá»ƒn thá»‹ lá»‹ch sá»­ há»™i thoáº¡i, nguá»“n tÃ i liá»‡u liÃªn quan.
- Trang admin:
  - Xem toÃ n bá»™ chunk trong vectorstore, click Ä‘á»ƒ xem chi tiáº¿t ná»™i dung vÃ  metadata.
  - XÃ³a toÃ n bá»™ vectorstore hoáº·c tá»«ng tÃ i liá»‡u.
  - Xem API docs, thá»­ API trá»±c tiáº¿p.

### 5. API vÃ  quáº£n trá»‹
- CÃ³ trang API Explorer (`/api-docs`) tá»± Ä‘á»™ng liá»‡t kÃª vÃ  cho phÃ©p thá»­ cÃ¡c endpoint.
- CÃ¡c API chÃ­nh: upload, chat, láº¥y danh sÃ¡ch tÃ i liá»‡u, debug vectorstore, xÃ³a vectorstore, xÃ³a tÃ i liá»‡u, láº¥y lá»‹ch sá»­ chat, ...

### 6. Lá»‹ch sá»­ há»™i thoáº¡i vÃ  session
- Lá»‹ch sá»­ há»™i thoáº¡i Ä‘Æ°á»£c lÆ°u trong session Flask (cookie).
- Khi refresh hoáº·c Ä‘á»•i session, lá»‹ch sá»­ sáº½ bá»‹ xÃ³a.
- Khi gá»­i cÃ¢u há»i, 10 lÆ°á»£t há»™i thoáº¡i gáº§n nháº¥t sáº½ Ä‘Æ°á»£c truyá»n vÃ o prompt Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh.

### 7. Tá»‘i Æ°u vÃ  báº£o trÃ¬
- CÃ³ thá»ƒ Ä‘á»•i model embedding, chunk_size, chunk_overlap trong code/config.
- CÃ³ thá»ƒ xem/log prompt gá»­i lÃªn LLM Ä‘á»ƒ debug.
- CÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm cÃ¡c chá»©c nÄƒng quáº£n trá»‹, tÃ¬m kiáº¿m, phÃ¢n quyá»n, ...