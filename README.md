# RAG Chatbot

A modern web-based chatbot using Retrieval-Augmented Generation (RAG) architecture with support for multiple LLM providers.

## Features

- ü§ñ **RAG Architecture**: Retrieval-Augmented Generation for context-aware responses
- üìÑ **Multi-format Support**: Upload and process PDF, DOCX, and TXT files
- üß† **Multiple LLM Providers**: 
  - Google Gemini (Cloud)
  - Local models via LM Studio (phi-2, Gemma, etc.)
- üé® **Modern UI**: Clean, responsive interface with dark mode support, beautiful chat bubbles, avatars, typing status, and real-time progress
- üìä **Vector Storage**: Chroma vector store for efficient document retrieval
- üîç **Document Management**: Upload, view, and manage your knowledge base
- üõ†Ô∏è **Admin Dashboard**: View/delete vectorstore, manage uploads, view chunk details
- üß© **API Explorer**: Interactive API docs and testing
- üåè **Vietnamese default answers**: Optimized for Vietnamese context
- üìù **Prompt logging, source deduplication, multi-chunk synthesis**

## Project Structure

```
RAG_ChatBot_01/
‚îú‚îÄ‚îÄ main.py                 # Flask application entry point, all routes
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ env.example            # Environment variables template
‚îú‚îÄ‚îÄ config.py               # App configuration
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ llm_provider.py    # LLM provider management (Gemini, Local, ...)
‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py # Document processing, chunking, file parsing
‚îÇ   ‚îî‚îÄ‚îÄ vector_store.py    # Vector store operations (ChromaDB)
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ index.html         # Main chat UI
‚îÇ   ‚îú‚îÄ‚îÄ admin.html         # Admin dashboard (view/delete DB, chunk details)
‚îÇ   ‚îú‚îÄ‚îÄ api_docs.html      # API Explorer (Swagger-like)
‚îÇ   ‚îî‚îÄ‚îÄ test_upload.html   # (Optional) Test upload UI
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ style.css          # All CSS (modern, responsive, dark mode)
‚îÇ   ‚îî‚îÄ‚îÄ main.js            # All JS (chat, upload, progress, admin, ...)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ uploads/           # Uploaded files
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/       # Chroma vector store (persisted DB)
‚îÇ   ‚îî‚îÄ‚îÄ sample/            # Sample documents for testing
‚îÇ       ‚îú‚îÄ‚îÄ ai_technologies.txt
‚îÇ       ‚îî‚îÄ‚îÄ sample_document.txt
‚îú‚îÄ‚îÄ logoBSR.png            # Company logo (used in sidebar, AI avatar)
‚îî‚îÄ‚îÄ README.md
```

## UI/UX Highlights
- **Modern chat bubbles**: Wide, readable, responsive, source files shown below each answer
- **Progress bar**: Shows both upload and chunking progress, with cancel/continue on page close
- **Sidebar**: LLM/model selection, DB/model status, document/chunk count, clear all
- **Admin dashboard**: View/delete vectorstore, see all chunks, chunk details, delete by document
- **API Explorer**: Live API docs and testing
- **Dark mode**: Toggle with one click
- **Company logo**: Sidebar and AI avatar
- **Vietnamese default answers**: Optimized for Vietnamese context

### UI Screenshots (add your own images in docs/)

```markdown
![Chat UI](docs/ui_chat.png)
![Sidebar](docs/sidebar.png)
![Admin Dashboard](docs/admin_dashboard.png)
![API Explorer](docs/api_explorer.png)
```

---

## How it works (Pipeline)

### Pipeline Diagram (Mermaid)

```mermaid
graph TD
    A[User Uploads File] --> B[Backend: Save & Chunk]
    B --> C[Embedding Model]
    C --> D[Chroma Vectorstore]
    E[User Asks Question] --> F[Retrieve Relevant Chunks]
    F --> G[Build Prompt]
    G --> H[LLM Gemini/Local]
    H --> I[Response + Sources]
    I --> J[Frontend: Display Chat + Sources]
```

---

### 1. Upload & Process Documents
- User uploads PDF, DOCX, or TXT files via the web UI.
- Backend saves files to `data/uploads/`.
- Each file is parsed and split into text chunks (configurable chunk size/overlap).

### 2. Embedding & Vectorstore
- Each chunk is embedded using a model (e.g. `intfloat/multilingual-e5-large`).
- Embeddings + metadata (file name, position, ...) are stored in ChromaDB (`data/vectorstore/`).

### 3. Chat & Retrieval
- User sends a question via chat UI.
- Backend retrieves top relevant chunks (semantic search) from vectorstore.
- Special keyword chunks are auto-merged for context.
- Last 10 chat turns are included for context.
- Prompt is constructed (context + history + question) and sent to LLM (Gemini or Local).
- LLM response is returned, formatted, and sources are deduplicated.

### 4. Frontend Display & Management
- Modern chat UI: chat bubbles, avatars, typing status, Enter to send, Shift+Enter for newline.
- Upload progress bar (with chunking progress), cancel/continue on page close.
- Sidebar: LLM/model selection, document/chunk count, DB/model status, clear all button.
- Admin dashboard: view/delete vectorstore, see all chunks, chunk details, delete by document.
- API Explorer: interactive docs and live API testing.

### 5. API & Admin
- `/api-docs`: API Explorer (auto-generated docs, live test)
- `/admin`: Admin dashboard (view/delete DB, chunk details)
- Main APIs: upload, chat, documents, vectorstore status, clear vectorstore, delete document, chat history, ...

### 6. Session & History
- Chat history is stored in Flask session (cookie-based, per user).
- Last 10 turns are used for context in prompt.
- History is cleared on refresh or new session.

## Sample Data
- Sample files in `data/sample/` for quick testing.
- You can upload these to see how the system works.

## Installation & Usage

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd RAG_ChatBot_01
   ```
2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
4. **Set up environment variables**
   ```bash
   cp env.example .env
   # Edit .env with your API keys and configuration
   ```
5. **Start the application**
   ```bash
   python main.py
   ```
6. **Open your browser**
   - Go to `http://localhost:5000`

## Configuration

- All config via `.env` or `config.py` (see `env.example` for all options)
- Key options:
  - `GOOGLE_API_KEY`: Gemini API key
  - `LOCAL_LLM_ENDPOINT`: LM Studio endpoint
  - `VECTOR_STORE_PATH`: Path to ChromaDB
  - `UPLOAD_FOLDER`: Where uploads are stored
  - `MAX_FILE_SIZE`: Max upload size (default 50MB)

## API Endpoints (Main)
- `GET /` - Main chat UI
- `POST /upload` - Upload and process documents (returns doc_id, triggers chunking)
- `GET /processing-status?doc_id=...` - Get chunking progress
- `POST /chat` - Chat with RAG bot
- `GET /documents` - List uploaded documents
- `POST /clear-vectorstore` - Delete all vectorstore data
- `GET /vectorstore-status` - Get DB/model status, doc/chunk count
- `GET /history` - Get chat history
- `POST /clear-history` - Clear chat history

## Development & Customization
- Add new LLM: Extend `backend/llm_provider.py`
- Add new document type: Extend `backend/document_loader.py`
- Change chunking/embedding: Edit `config.py` or `document_loader.py`
- UI/UX: Edit `templates/index.html`, `static/style.css`, `static/main.js`
- Admin/API: Edit `templates/admin.html`, `templates/api_docs.html`

## Troubleshooting
- **Upload l·ªói 413**: TƒÉng `MAX_CONTENT_LENGTH` trong Flask v√† proxy (Nginx/Apache)
- **Kh√¥ng nh·∫≠n model local**: Ki·ªÉm tra LM Studio ƒë√£ ch·∫°y v√† endpoint ƒë√∫ng
- **Gemini l·ªói 404/401**: Ki·ªÉm tra API key v√† quota
- **Vectorstore l·ªói**: X√≥a th∆∞ m·ª•c `data/vectorstore/` ƒë·ªÉ reset

## License
MIT License

## Acknowledgments
- [LangChain](https://langchain.com/) for RAG framework
- [Chroma](https://www.trychroma.com/) for vector storage
- [LM Studio](https://lmstudio.ai/) for local LLM hosting
- [Google Gemini](https://ai.google.dev/) for cloud LLM access

## C√°ch ho·∫°t ƒë·ªông c·ªßa web app

### 1. Upload v√† x·ª≠ l√Ω t√†i li·ªáu
- Ng∆∞·ªùi d√πng upload file PDF, DOCX, TXT qua giao di·ªán web.
- Backend l∆∞u file v√†o th∆∞ m·ª•c `data/uploads/`.
- File ƒë∆∞·ª£c ƒë·ªçc v√† chia nh·ªè th√†nh c√°c chunk (theo c·∫•u h√¨nh chunk_size, chunk_overlap).

### 2. Sinh embedding v√† l∆∞u vectorstore
- M·ªói chunk ƒë∆∞·ª£c chuy·ªÉn th√†nh vector embedding b·∫±ng model (v√≠ d·ª•: `intfloat/multilingual-e5-large`).
- C√°c embedding v√† metadata (t√™n file, v·ªã tr√≠, lo·∫°i file, ...) ƒë∆∞·ª£c l∆∞u v√†o vector database (ChromaDB) trong `data/vectorstore/`.

### 3. Truy v·∫•n v√† sinh c√¢u tr·∫£ l·ªùi
- Khi ng∆∞·ªùi d√πng g·ª≠i c√¢u h·ªèi:
  1. Backend l·∫•y 12 chunk li√™n quan nh·∫•t (theo embedding) t·ª´ vectorstore.
  2. T·ª± ƒë·ªông t√¨m th√™m c√°c chunk ch·ª©a t·ª´ kh√≥a ƒë·∫∑c bi·ªát trong c√¢u h·ªèi (v√≠ d·ª•: "208HV", "NMLD") ƒë·ªÉ gh√©p v√†o context.
  3. L·∫•y 10 l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t t·ª´ session ƒë·ªÉ truy·ªÅn v√†o prompt.
  4. Gh√©p context (c√≥ c·∫Øt chunk t·ªëi ƒëa 2000 k√Ω t·ª±), l·ªãch s·ª≠ h·ªôi tho·∫°i, v√† c√¢u h·ªèi th√†nh prompt.
  5. G·ª≠i prompt n√†y l√™n LLM (Gemini ho·∫∑c local LLM qua LM Studio).
  6. Nh·∫≠n c√¢u tr·∫£ l·ªùi, format HTML ƒë·∫πp.
  7. Tr·∫£ v·ªÅ frontend c√πng danh s√°ch ngu·ªìn (file) duy nh·∫•t.

### 4. Hi·ªÉn th·ªã v√† qu·∫£n l√Ω tr√™n frontend
- Giao di·ªán chat hi·ªán ƒë·∫°i, h·ªó tr·ª£:
  - G·ª≠i tin nh·∫Øn b·∫±ng Enter, xu·ªëng d√≤ng b·∫±ng Shift+Enter.
  - Ch·ªçn LLM, model Gemini ƒë·ªông.
  - Upload t√†i li·ªáu, xem danh s√°ch t√†i li·ªáu ƒë√£ upload.
  - Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i, ngu·ªìn t√†i li·ªáu li√™n quan.
- Trang admin:
  - Xem to√†n b·ªô chunk trong vectorstore, click ƒë·ªÉ xem chi ti·∫øt n·ªôi dung v√† metadata.
  - X√≥a to√†n b·ªô vectorstore ho·∫∑c t·ª´ng t√†i li·ªáu.
  - Xem API docs, th·ª≠ API tr·ª±c ti·∫øp.

### 5. API v√† qu·∫£n tr·ªã
- C√≥ trang API Explorer (`/api-docs`) t·ª± ƒë·ªông li·ªát k√™ v√† cho ph√©p th·ª≠ c√°c endpoint.
- C√°c API ch√≠nh: upload, chat, l·∫•y danh s√°ch t√†i li·ªáu, debug vectorstore, x√≥a vectorstore, x√≥a t√†i li·ªáu, l·∫•y l·ªãch s·ª≠ chat, ...

### 6. L·ªãch s·ª≠ h·ªôi tho·∫°i v√† session
- L·ªãch s·ª≠ h·ªôi tho·∫°i ƒë∆∞·ª£c l∆∞u trong session Flask (cookie).
- Khi refresh ho·∫∑c ƒë·ªïi session, l·ªãch s·ª≠ s·∫Ω b·ªã x√≥a.
- Khi g·ª≠i c√¢u h·ªèi, 10 l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t s·∫Ω ƒë∆∞·ª£c truy·ªÅn v√†o prompt ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh.

### 7. T·ªëi ∆∞u v√† b·∫£o tr√¨
- C√≥ th·ªÉ ƒë·ªïi model embedding, chunk_size, chunk_overlap trong code/config.
- C√≥ th·ªÉ xem/log prompt g·ª≠i l√™n LLM ƒë·ªÉ debug.
- C√≥ th·ªÉ m·ªü r·ªông th√™m c√°c ch·ª©c nƒÉng qu·∫£n tr·ªã, t√¨m ki·∫øm, ph√¢n quy·ªÅn, ...